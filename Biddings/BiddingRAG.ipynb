{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf99560-2b17-40a0-864e-eaa66f33fb2f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6358fd6b-8324-421a-b207-32d8a12f43cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\eval\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\envs\\eval\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-10 16:31:26,092 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: D:\\modelscope\\models\\Qwen\\Qwen3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 16:31:27,533 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: D:\\modelscope\\models\\Qwen\\Qwen3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:16<00:00,  3.31s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from modelscope import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = 'Qwen/Qwen3-8B'\n",
    "# model_id = 'Qwen/Qwen3-0.6B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype=torch.float16).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5977bae5-8601-45b2-be6c-c029918b6ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def qwen_llm(prompt):\n",
    "#     inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=32768)\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def qwen_llm(prompt_str: str) -> str:\n",
    "    print(\"Qwen...\")\n",
    "    if hasattr(prompt_str, 'to_string'):\n",
    "        prompt_str = prompt_str.to_string()\n",
    "    # âœ… ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²\n",
    "    assert isinstance(prompt_str, str), f\"Expected string, got {type(prompt_str)}\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt_str}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=True\n",
    "        )\n",
    "    inputs = tokenizer([text], return_tensors='pt').to(qwen_model.device)\n",
    "    outputs = qwen_model.generate(**inputs, max_new_tokens=32768)  \n",
    "    # return tokenizer.decode(outputs[0], skip_special_tokens=True).split('<think>\\n\\n</think>\\n\\n')[-1]\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67a466e-c6a6-402e-824e-71ea8d40c8be",
   "metadata": {},
   "source": [
    "## Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7017eb5-cbfd-405d-af26-3a8104e8bdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, Collection\n",
    "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "col_name = \"AllBiddings\"\n",
    "\n",
    "col = Collection(col_name)\n",
    "col.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13452ca8-0d79-4003-80f5-a4f4e321ec54",
   "metadata": {},
   "source": [
    "## Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "793fd1ad-61bf-49fa-b3d4-eb7c44030e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel('BAAI/bge-m3',\n",
    "                      use_fp16=False,\n",
    "                      pooling_method='cls',\n",
    "                      devices=['cuda:0'])\n",
    "\n",
    "def get_embeddings(text):\n",
    "    embeddings = model.encode(\n",
    "        text,\n",
    "        return_dense=True,\n",
    "        return_sparse=True,\n",
    "        return_colbert_vecs=False\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1fab10-6986-4f20-a60b-76e0f639c5ae",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e18abf2b-81b6-442a-a2bb-cf17bfc00307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import AnnSearchRequest, WeightedRanker\n",
    "\n",
    "def hybrid_search(\n",
    "    col,\n",
    "    query_dense_embedding,\n",
    "    query_sparse_embedding,\n",
    "    sparse_weight=1.0,\n",
    "    dense_weight=1.0,\n",
    "    limit=5,\n",
    "):\n",
    "    print(\"Searching...\")\n",
    "    dense_req = AnnSearchRequest(\n",
    "        [query_dense_embedding], \"dense_vector\", {\"metric_type\": \"L2\", \"params\": {}}, limit=limit\n",
    "    )\n",
    "    sparse_req = AnnSearchRequest(\n",
    "        [query_sparse_embedding], \"sparse_vector\", {\"metric_type\": \"IP\", \"params\": {}}, limit=limit\n",
    "    )\n",
    "    rerank = WeightedRanker(sparse_weight, dense_weight)\n",
    "    res = col.hybrid_search(\n",
    "        [sparse_req, dense_req],\n",
    "        rerank=rerank,\n",
    "        limit=limit,\n",
    "        output_fields=[\"text\"]\n",
    "    )\n",
    "    return [\n",
    "        {\"text\": hit.entity.get(\"text\")}\n",
    "        for hit in res[0]\n",
    "    ]\n",
    "\n",
    "def hybrid_search_pipeline(query):\n",
    "    # åœ¨è¿™é‡Œè°ƒç”¨ä½ çš„ embedding æ¨¡å‹ï¼ˆæ¯”å¦‚ bge-m3ï¼‰\n",
    "    print(\"Embedding...\")\n",
    "    query_embeddings = get_embeddings([query])\n",
    "    query_dense_embeddings = query_embeddings['dense_vecs'][0]\n",
    "    query_sparse_embeddings = query_embeddings.get('lexical_weights')[0]\n",
    "\n",
    "    # è°ƒç”¨åŸå§‹çš„æœç´¢æ–¹æ³•\n",
    "    return hybrid_search(\n",
    "        col,\n",
    "        query_dense_embeddings,\n",
    "        query_sparse_embeddings,\n",
    "        sparse_weight=1.0,\n",
    "        dense_weight=1.0,\n",
    "        limit=50\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1997e365-4a55-4eb6-8ba2-d2a1767b928f",
   "metadata": {},
   "source": [
    "## Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c0eb23d-ab68-49f0-97e6-696eb82fdcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus.model.reranker import BGERerankFunction\n",
    "\n",
    "bge_rf = BGERerankFunction(\n",
    "    model_name=\"BAAI/bge-reranker-v2-m3\",  # Specify the model name. Defaults to `BAAI/bge-reranker-v2-m3`.\n",
    "    device=\"cuda:0\" # Specify the device to use, e.g., 'cpu' or 'cuda:0'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d0f42e5-738a-4513-a37e-a178be5251d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(results):\n",
    "    print(\"Reranking...\")\n",
    "    documents = [p['text'] for p in results]\n",
    "    rerank_results = bge_rf(query=query, documents=documents, top_k=1)\n",
    "    return rerank_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a92806c-0daf-4d22-8b84-7d853088cffb",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "beff761e-c431-4a8b-b558-f8177fca13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Human: You are an AI assistant, and provides answers to questions by using fact based and statistical information when possible.\n",
    "Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\n",
    "If users ask logical question rather than contexual question, you 'd better provide the url of project to avoid protential mistake.\n",
    "If the question is an inference question, you need to inferent step by step.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible. \n",
    "At last you may remind user to get more info from provided url.\n",
    "Answer in Chinese.\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "# Create a PromptTemplate instance with the defined template and input variables\n",
    "prompt = PromptTemplate(\n",
    "    template=PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Define a function to format the retrieved documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.text for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d48f41e-292f-45b5-b20e-7af903399528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "context_chain = RunnableLambda(hybrid_search_pipeline) | RunnableLambda(rerank) | RunnableLambda(format_docs)\n",
    "llm = RunnableLambda(qwen_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cefac7-2c87-4ce9-9f84-5c54f72f8dd1",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06b1b5dc-dda6-4895-b5d9-d792d0151d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your search query:  æœ€æ–°ä¸‰æ¡æ”¿åºœé‡‡è´­ä¿¡æ¯æ˜¯ä»€ä¹ˆï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter your search query: \")\n",
    "# query_embeddings = get_embeddings([query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b77cd7c-f547-42f9-8efc-d409789c928b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding...\n",
      "Searching...\n",
      "Reranking...\n",
      "Qwen...\n",
      "user\n",
      "\n",
      "Human: You are an AI assistant, and provides answers to questions by using fact based and statistical information when possible.\n",
      "Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\n",
      "If users ask logical question rather than contexual question, you 'd better provide the url of project to avoid protential mistake.\n",
      "If the question is an inference question, you need to inferent step by step.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "<context>\n",
      "é¡¹ç›®ç¼–å·ï¼š310107000250320195232-07224789ã€‚å…¬å‘Šæ ‡é¢˜ï¼šé‡‡è´­æ‚¦å¿ƒäº­å¿ƒç†æœåŠ¡äº­è½¯ä»¶ä¸‰æœŸé¡¹ç›®çš„ä¸­æ ‡ï¼ˆæˆäº¤ï¼‰ç»“æœå…¬å‘Šã€‚é¡¹ç›®åç§°ï¼šé‡‡è´­æ‚¦å¿ƒäº­å¿ƒç†æœåŠ¡äº­è½¯ä»¶ä¸‰æœŸé¡¹ç›®ã€‚é‡‡è´­é¡¹ç›®å­ç¼–å·ï¼š1ã€‚æ ‡é¡¹åç§°ï¼šé‡‡è´­æ‚¦å¿ƒäº­å¿ƒç†æœåŠ¡äº­è½¯ä»¶ä¸‰æœŸé¡¹ç›®ã€‚ä»£ç†æœºæ„åç§°ï¼šä¸Šæµ·å¸‚æ™®é™€åŒºæ”¿åºœé‡‡è´­ä¸­å¿ƒã€‚ä»£ç†æœºæ„ä»£ç ï¼š12310107764741781Bã€‚ä¸­æ ‡é‡‘é¢ï¼š1630000.00ã€‚ä¸­æ ‡ä¾›åº”å•†åç§°ï¼šä¸Šæµ·å›­æˆåŒ»ç–—å™¨æ¢°æœ‰é™å…¬å¸ã€‚ä¸­æ ‡ä¾›åº”å•†åœ°å€ï¼šä¸Šæµ·å¸‚å®å±±åŒºé€¸ä»™è·¯2816å·1å¹¢9å±‚A0902å®¤ã€‚å¾—åˆ†ï¼š85.27ã€‚é“¾æ¥ï¼šhttps://www.shggzy.com/jyxxzcgs/8231765?cExt=eyJhbGciOiJIUzI1NiJ9.eyJwYXRoIjoiL2p5eHh6YyIsInBhZ2VObyI6MSwiZXhwIjoxNzQ4MjY1NzgwMTIzfQ.U0z7uqmiYYh_loqBm8V-z4zQw1USwzE6x8KWrq-Ihxo&amp;isIndex=ã€‚çˆ¬å–æ—¶é—´ï¼šnanã€‚\n",
      "\n",
      "é¡¹ç›®ç¼–å·ï¼š310104000241128151994-04183976ã€‚å…¬å‘Šæ ‡é¢˜ï¼šä¸Šæµ·å¸‚å¾æ±‡åŒºæ”¿åºœé‡‡è´­ä¸­å¿ƒâ€”â€”ä¸Šæµ·å¸‚å¾æ±‡åŒºå«ç”Ÿäº‹ä¸šç®¡ç†å‘å±•ä¸­å¿ƒä¸‰æ±Ÿè·¯ç‰©ä¸šç®¡ç†æœåŠ¡æ”¿åºœé‡‡è´­é¡¹ç›®çš„ä¸­æ ‡ï¼ˆæˆäº¤ï¼‰ç»“æœå…¬å‘Šã€‚é¡¹ç›®åç§°ï¼šä¸Šæµ·å¸‚å¾æ±‡åŒºæ”¿åºœé‡‡è´­ä¸­å¿ƒâ€”â€”ä¸Šæµ·å¸‚å¾æ±‡åŒºå«ç”Ÿäº‹ä¸šç®¡ç†å‘å±•ä¸­å¿ƒä¸‰æ±Ÿè·¯ç‰©ä¸šç®¡ç†æœåŠ¡æ”¿åºœé‡‡è´­é¡¹ç›®ã€‚é‡‡è´­é¡¹ç›®å­ç¼–å·ï¼š1ã€‚æ ‡é¡¹åç§°ï¼šä¸Šæµ·å¸‚å¾æ±‡åŒºå«ç”Ÿäº‹ä¸šç®¡ç†å‘å±•ä¸­å¿ƒä¸‰æ±Ÿè·¯ç‰©ä¸šç®¡ç†æœåŠ¡ã€‚ä»£ç†æœºæ„åç§°ï¼šå¾æ±‡åŒºæ”¿åºœé‡‡è´­ä¸­å¿ƒã€‚ä»£ç†æœºæ„ä»£ç ï¼š123101044251161729ã€‚ä¸­æ ‡é‡‘é¢ï¼š4480920.00ã€‚ä¸­æ ‡ä¾›åº”å•†åç§°ï¼šä¸Šæµ·æ±‡æˆç‰©ä¸šæœ‰é™å…¬å¸ã€‚ä¸­æ ‡ä¾›åº”å•†åœ°å€ï¼šå¾æ±‡åŒºæ¼•ä¸œæ”¯è·¯81å·ã€‚å¾—åˆ†ï¼š93.3ã€‚é“¾æ¥ï¼šhttps://www.shggzy.com/jyxxzcgs/8210440?cExt=eyJhbGciOiJIUzI1NiJ9.eyJwYXRoIjoiL2p5eHh6YyIsInBhZ2VObyI6MSwiZXhwIjoxNzQ4MjY1NzgwMTIzfQ.U0z7uqmiYYh_loqBm8V-z4zQw1USwzE6x8KWrq-Ihxo&amp;isIndex=ã€‚çˆ¬å–æ—¶é—´ï¼šnanã€‚\n",
      "\n",
      "é¡¹ç›®ç¼–å·ï¼š310113000250110159503-13185605ã€‚å…¬å‘Šæ ‡é¢˜ï¼šæ”¿åºœè´­ä¹°æœåŠ¡äººå‘˜ç»è´¹çš„ä¸­æ ‡ï¼ˆæˆäº¤ï¼‰ç»“æœå…¬å‘Šã€‚é¡¹ç›®åç§°ï¼šæ”¿åºœè´­ä¹°æœåŠ¡äººå‘˜ç»è´¹ã€‚é‡‡è´­é¡¹ç›®å­ç¼–å·ï¼š1ã€‚æ ‡é¡¹åç§°ï¼šæ”¿åºœè´­ä¹°æœåŠ¡äººå‘˜ç»è´¹ã€‚ä»£ç†æœºæ„åç§°ï¼šä¸Šæµ·ç‘å’Œå·¥ç¨‹å’¨è¯¢æœ‰é™å…¬å¸ã€‚ä»£ç†æœºæ„ä»£ç ï¼š913101137575954641ã€‚ä¸­æ ‡é‡‘é¢ï¼š4349960.92ã€‚ä¸­æ ‡ä¾›åº”å•†åç§°ï¼šä¸Šæµ·åšéœ–äººåŠ›èµ„æºæœ‰é™å…¬å¸ã€‚ä¸­æ ‡ä¾›åº”å•†åœ°å€ï¼šä¸Šæµ·å¸‚å®å±±åŒºå®å®‰å…¬è·¯857å·åšéœ–ç§‘åˆ›äººæ‰äº§ä¸šå›­ã€‚å¾—åˆ†ï¼š90.32ã€‚é“¾æ¥ï¼šhttps://www.shggzy.com/jyxxzcgs/8216029?cExt=eyJhbGciOiJIUzI1NiJ9.eyJwYXRoIjoiL2p5eHh6YyIsInBhZ2VObyI6MSwiZXhwIjoxNzQ4MjY1NzgwMTIzfQ.U0z7uqmiYYh_loqBm8V-z4zQw1USwzE6x8KWrq-Ihxo&amp;isIndex=ã€‚çˆ¬å–æ—¶é—´ï¼šnanã€‚\n",
      "\n",
      "é¡¹ç›®ç¼–å·ï¼š310000000241223157018-00191858ã€‚å…¬å‘Šæ ‡é¢˜ï¼šâ€œä¸‰åŒºä¸‰çº¿â€å®æ–½ç›‘æµ‹è¯„ä¼°å’ŒåŠ¨æ€ç»´æŠ¤çš„ä¸­æ ‡ï¼ˆæˆäº¤ï¼‰ç»“æœå…¬å‘Šã€‚é¡¹ç›®åç§°ï¼šâ€œä¸‰åŒºä¸‰çº¿â€å®æ–½ç›‘æµ‹è¯„ä¼°å’ŒåŠ¨æ€ç»´æŠ¤ã€‚é‡‡è´­é¡¹ç›®å­ç¼–å·ï¼š1ã€‚æ ‡é¡¹åç§°ï¼šâ€œä¸‰åŒºä¸‰çº¿â€å®æ–½ç›‘æµ‹è¯„ä¼°å’ŒåŠ¨æ€ç»´æŠ¤ã€‚ä»£ç†æœºæ„åç§°ï¼šä¸Šæµ·è´¢ç‘å»ºè®¾ç®¡ç†æœ‰é™å…¬å¸ã€‚ä»£ç†æœºæ„ä»£ç ï¼š91310114324590721Qã€‚ä¸­æ ‡é‡‘é¢ï¼š1642500.00ã€‚ä¸­æ ‡ä¾›åº”å•†åç§°ï¼šä¸Šæµ·å¸‚è‡ªç„¶èµ„æºè°ƒæŸ¥åˆ©ç”¨ç ”ç©¶é™¢ã€‚ä¸­æ ‡ä¾›åº”å•†åœ°å€ï¼šä¸Šæµ·å¸‚é™å®‰åŒºçµçŸ³è·¯930å·åœ°è´¨å¤§å¦ã€‚å¾—åˆ†ï¼š90.64ã€‚é“¾æ¥ï¼šhttps://www.shggzy.com/jyxxzcgs/8184862?cExt=eyJhbGciOiJIUzI1NiJ9.eyJwYXRoIjoiL2p5eHh6YyIsInBhZ2VObyI6MSwiZXhwIjoxNzQ4MjY4MTc3NzA3fQ.BJ3H_Uuc2XZyGIr_S_qvIM45BwARcJSiwG5_It6sFsE&amp;isIndex=ã€‚çˆ¬å–æ—¶é—´ï¼šnanã€‚\n",
      "\n",
      "é¡¹ç›®ç¼–å·ï¼š310000000241111146101-00182451ã€‚å…¬å‘Šæ ‡é¢˜ï¼šä¸Šæµ·å¸‚ç”µåŠ¨è‡ªè¡Œè½¦å…¨é“¾æ¡å®‰å…¨ç›‘ç®¡ä¿¡æ¯å­ç³»ç»Ÿå»ºè®¾é¡¹ç›®çš„ä¸­æ ‡ï¼ˆæˆäº¤ï¼‰ç»“æœå…¬å‘Šã€‚é¡¹ç›®åç§°ï¼šä¸Šæµ·å¸‚ç”µåŠ¨è‡ªè¡Œè½¦å…¨é“¾æ¡å®‰å…¨ç›‘ç®¡ä¿¡æ¯å­ç³»ç»Ÿå»ºè®¾é¡¹ç›®ã€‚é‡‡è´­é¡¹ç›®å­ç¼–å·ï¼š1ã€‚æ ‡é¡¹åç§°ï¼šä¸Šæµ·å¸‚ç”µåŠ¨è‡ªè¡Œè½¦å…¨é“¾æ¡å®‰å…¨ç›‘ç®¡ä¿¡æ¯å­ç³»ç»Ÿå»ºè®¾é¡¹ç›®ã€‚ä»£ç†æœºæ„åç§°ï¼šä¸Šæµ·å¸‚æ”¿åºœé‡‡è´­ä¸­å¿ƒã€‚ä»£ç†æœºæ„ä»£ç ï¼š12310000425203938Bã€‚ä¸­æ ‡é‡‘é¢ï¼š3624000.00ã€‚ä¸­æ ‡ä¾›åº”å•†åç§°ï¼šä¸Šæµ·å¸‚å¤§æ•°æ®è‚¡ä»½æœ‰é™å…¬å¸ã€‚ä¸­æ ‡ä¾›åº”å•†åœ°å€ï¼šä¸Šæµ·ä¸Šæµ·å¸‚é™å®‰åŒºæ±Ÿåœºä¸‰è·¯228å·409å®¤ã€‚å¾—åˆ†ï¼š93.33ã€‚é“¾æ¥ï¼šhttps://www.shggzy.com/jyxxzcgs/8165624?cExt=eyJhbGciOiJIUzI1NiJ9.eyJwYXRoIjoiL2p5eHh6YyIsInBhZ2VObyI6MSwiZXhwIjoxNzQ4MjgwNTk5Nzc0fQ.0KOTrZZSrMdGtVYcYhYTBiOlPM0kmGdefKTRxABKoTM&amp;isIndex=ã€‚çˆ¬å–æ—¶é—´ï¼šnanã€‚\n",
      "</context>\n",
      "\n",
      "<question>\n",
      "æœ€æ–°ä¸‰æ¡æ”¿åºœé‡‡è´­ä¿¡æ¯æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "</question>\n",
      "\n",
      "The response should be specific and use statistics or numbers when possible. \n",
      "At last you may remind user to get more info from provided url.\n",
      "Answer in Chinese.\n",
      "\n",
      "Assistant:\n",
      "assistant\n",
      "<think>\n",
      "å¥½çš„ï¼Œæˆ‘éœ€è¦å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼šâ€œæœ€æ–°ä¸‰æ¡æ”¿åºœé‡‡è´­ä¿¡æ¯æ˜¯ä»€ä¹ˆï¼Ÿâ€é¦–å…ˆï¼Œæˆ‘éœ€è¦ä»”ç»†æŸ¥çœ‹æä¾›çš„ä¸Šä¸‹æ–‡ä¸­çš„é¡¹ç›®ä¿¡æ¯ï¼Œç¡®å®šå“ªäº›æ˜¯æœ€è¿‘çš„ã€‚\n",
      "\n",
      "é¦–å…ˆï¼Œç”¨æˆ·æä¾›çš„ä¸Šä¸‹æ–‡ä¸­æœ‰äº”ä¸ªé¡¹ç›®ï¼Œæ¯ä¸ªé¡¹ç›®éƒ½æœ‰ä¸åŒçš„é¡¹ç›®ç¼–å·å’Œçˆ¬å–æ—¶é—´ã€‚æ³¨æ„åˆ°æ‰€æœ‰é¡¹ç›®çš„çˆ¬å–æ—¶é—´éƒ½æ˜¯â€œnanâ€ï¼Œè¿™å¯èƒ½è¡¨ç¤ºæ•°æ®ç¼ºå¤±æˆ–æœªè®°å½•ã€‚ä¸è¿‡ï¼Œå¯èƒ½ç”¨æˆ·è®¤ä¸ºè¿™äº›é¡¹ç›®æ˜¯æŒ‰æ—¶é—´é¡ºåºæ’åˆ—çš„ï¼Œæˆ–è€…æ ¹æ®é¡¹ç›®ç¼–å·çš„æŸäº›éƒ¨åˆ†æ¥åˆ¤æ–­æ—¶é—´ã€‚\n",
      "\n",
      "é¡¹ç›®ç¼–å·çš„æ ¼å¼çœ‹èµ·æ¥æ˜¯â€œ310107000250320195232-07224789â€è¿™æ ·çš„ç»“æ„ï¼Œå¯èƒ½å‰åŠéƒ¨åˆ†æ˜¯å¹´ä»½æˆ–å…¶ä»–æ—¶é—´ç›¸å…³çš„ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œç¬¬ä¸€ä¸ªé¡¹ç›®çš„é¡¹ç›®ç¼–å·æ˜¯310107000250320195232ï¼Œå…¶ä¸­å¯èƒ½åŒ…å«å¹´ä»½â€œ2019â€ï¼Œä½†åé¢çš„é¡¹ç›®ç¼–å·å¦‚310104000241128151994å¯èƒ½åŒ…å«â€œ2015â€æˆ–â€œ2019â€ï¼Ÿéœ€è¦ä»”ç»†æ£€æŸ¥æ¯ä¸ªé¡¹ç›®ç¼–å·çš„æ—¥æœŸéƒ¨åˆ†ã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œå¯èƒ½æ›´ç®€å•çš„æ–¹æ³•æ˜¯æŒ‰ä¸Šä¸‹æ–‡ä¸­çš„é¡ºåºæ¥åˆ¤æ–­ï¼Œå› ä¸ºç”¨æˆ·å¯èƒ½å°†æœ€æ–°çš„æ”¾åœ¨æœ€åã€‚ä¾‹å¦‚ï¼Œç¬¬äº”ä¸ªé¡¹ç›®æ˜¯â€œä¸Šæµ·å¸‚ç”µåŠ¨è‡ªè¡Œè½¦å…¨é“¾æ¡å®‰å…¨ç›‘ç®¡ä¿¡æ¯å­ç³»ç»Ÿå»ºè®¾é¡¹ç›®â€ï¼Œé¡¹ç›®ç¼–å·ä¸º310000000241111146101-00182451ï¼Œå¯èƒ½æ—¥æœŸè¾ƒæ–°ã€‚åŒæ ·ï¼Œç¬¬å››ä¸ªé¡¹ç›®æ˜¯â€œä¸‰åŒºä¸‰çº¿â€é¡¹ç›®ï¼Œç¼–å·ä¸º310000000241223157018-00191858ï¼Œç¬¬ä¸‰ä¸ªé¡¹ç›®æ˜¯æ”¿åºœè´­ä¹°æœåŠ¡äººå‘˜ç»è´¹ï¼Œç¼–å·ä¸º310113000250110159503-13185605ï¼Œç¬¬äºŒä¸ªæ˜¯å¾æ±‡åŒºçš„ç‰©ä¸šç®¡ç†æœåŠ¡ï¼Œç¼–å·ä¸º310104000241128151994-04183976ï¼Œç¬¬ä¸€ä¸ªæ˜¯æ‚¦å¿ƒäº­å¿ƒç†æœåŠ¡äº­è½¯ä»¶é¡¹ç›®ï¼Œç¼–å·ä¸º310107000250320195232-07224789ã€‚\n",
      "\n",
      "ä½†é¡¹ç›®ç¼–å·ä¸­çš„æ—¥æœŸéƒ¨åˆ†å¯èƒ½éœ€è¦è§£æã€‚ä¾‹å¦‚ï¼Œé¡¹ç›®ç¼–å·ä¸­çš„å‰å‡ ä½æ•°å­—å¯èƒ½ä»£è¡¨å¹´ä»½ã€‚ä¾‹å¦‚ï¼Œç¬¬ä¸€ä¸ªé¡¹ç›®ç¼–å·310107000250320195232ï¼Œå…¶ä¸­â€œ2019â€å¯èƒ½å‡ºç°åœ¨æŸä¸ªä½ç½®ï¼Œä½†ä¸ç¡®å®šã€‚æˆ–è€…å¯èƒ½é¡¹ç›®ç¼–å·ä¸­çš„åå‡ ä½æ•°å­—ä»£è¡¨å¹´ä»½ï¼Œæ¯”å¦‚â€œ320195232â€ä¸­çš„â€œ2019â€ï¼Ÿè¿™å¯èƒ½ä¸å¤ªå‡†ç¡®ã€‚ç”±äºæ—¶é—´è§£æå›°éš¾ï¼Œå¯èƒ½éœ€è¦å‡è®¾ä¸Šä¸‹æ–‡ä¸­çš„é¡ºåºå³ä¸ºæ—¶é—´é¡ºåºï¼Œå³æœ€åä¸‰ä¸ªé¡¹ç›®æ˜¯æœ€è¿‘çš„ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œæœ€æ–°çš„ä¸‰æ¡æ”¿åºœé‡‡è´­ä¿¡æ¯åº”è¯¥æ˜¯ï¼š\n",
      "\n",
      "1. ä¸Šæµ·å¸‚ç”µåŠ¨è‡ªè¡Œè½¦å…¨é“¾æ¡å®‰å…¨ç›‘ç®¡ä¿¡æ¯å­ç³»ç»Ÿå»ºè®¾é¡¹ç›®ï¼Œä¸­æ ‡é‡‘é¢3624000.00ï¼Œä¸­æ ‡ä¾›åº”å•†æ˜¯ä¸Šæµ·å¸‚å¤§æ•°æ®è‚¡ä»½æœ‰é™å…¬å¸ã€‚\n",
      "2. â€œä¸‰åŒºä¸‰çº¿â€å®æ–½ç›‘æµ‹è¯„ä¼°å’ŒåŠ¨æ€ç»´æŠ¤é¡¹ç›®ï¼Œä¸­æ ‡é‡‘é¢1642500.00ï¼Œä¸­æ ‡ä¾›åº”å•†æ˜¯ä¸Šæµ·å¸‚è‡ªç„¶èµ„æºè°ƒæŸ¥åˆ©ç”¨ç ”ç©¶é™¢ã€‚\n",
      "3. æ”¿åºœè´­ä¹°æœåŠ¡äººå‘˜ç»è´¹é¡¹ç›®ï¼Œä¸­æ ‡é‡‘é¢4349960.92ï¼Œä¸­æ ‡ä¾›åº”å•†æ˜¯ä¸Šæµ·åšéœ–äººåŠ›èµ„æºæœ‰é™å…¬å¸ã€‚\n",
      "\n",
      "éœ€è¦ç¡®è®¤è¿™äº›æ˜¯å¦ç¡®å®æ˜¯æœ€æ–°ï¼Œä½†ç”±äºçˆ¬å–æ—¶é—´éƒ½æ˜¯nanï¼Œå¯èƒ½æ— æ³•å‡†ç¡®åˆ¤æ–­ï¼Œä½†æ ¹æ®ä¸Šä¸‹æ–‡çš„æ’åˆ—é¡ºåºï¼Œæœ€åä¸‰ä¸ªå¯èƒ½æ˜¯æœ€è¿‘çš„ã€‚å¦å¤–ï¼Œç”¨æˆ·å¯èƒ½éœ€è¦é€šè¿‡æä¾›çš„é“¾æ¥è·å–æ›´å¤šä¿¡æ¯ï¼Œå› æ­¤æœ€åæé†’ç”¨æˆ·æŸ¥çœ‹é“¾æ¥ã€‚\n",
      "</think>\n",
      "\n",
      "æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæœ€æ–°çš„ä¸‰æ¡æ”¿åºœé‡‡è´­ä¿¡æ¯å¦‚ä¸‹ï¼š  \n",
      "\n",
      "1. **ä¸Šæµ·å¸‚ç”µåŠ¨è‡ªè¡Œè½¦å…¨é“¾æ¡å®‰å…¨ç›‘ç®¡ä¿¡æ¯å­ç³»ç»Ÿå»ºè®¾é¡¹ç›®**  \n",
      "   - ä¸­æ ‡é‡‘é¢ï¼š3,624,000.00å…ƒ  \n",
      "   - ä¸­æ ‡ä¾›åº”å•†ï¼šä¸Šæµ·å¸‚å¤§æ•°æ®è‚¡ä»½æœ‰é™å…¬å¸  \n",
      "   - é¡¹ç›®é“¾æ¥ï¼š[ç‚¹å‡»æŸ¥çœ‹](https://www.shggzy.com/jyxxzcgs/8165624)  \n",
      "\n",
      "2. **â€œä¸‰åŒºä¸‰çº¿â€å®æ–½ç›‘æµ‹è¯„ä¼°å’ŒåŠ¨æ€ç»´æŠ¤é¡¹ç›®**  \n",
      "   - ä¸­æ ‡é‡‘é¢ï¼š1,642,500.00å…ƒ  \n",
      "   - ä¸­æ ‡ä¾›åº”å•†ï¼šä¸Šæµ·å¸‚è‡ªç„¶èµ„æºè°ƒæŸ¥åˆ©ç”¨ç ”ç©¶é™¢  \n",
      "   - é¡¹ç›®é“¾æ¥ï¼š[ç‚¹å‡»æŸ¥çœ‹](https://www.shggzy.com/jyxxzcgs/8184862)  \n",
      "\n",
      "3. **æ”¿åºœè´­ä¹°æœåŠ¡äººå‘˜ç»è´¹é¡¹ç›®**  \n",
      "   - ä¸­æ ‡é‡‘é¢ï¼š4,349,960.92å…ƒ  \n",
      "   - ä¸­æ ‡ä¾›åº”å•†ï¼šä¸Šæµ·åšéœ–äººåŠ›èµ„æºæœ‰é™å…¬å¸  \n",
      "   - é¡¹ç›®é“¾æ¥ï¼š[ç‚¹å‡»æŸ¥çœ‹](https://www.shggzy.com/jyxxzcgs/8216029)  \n",
      "\n",
      "ä»¥ä¸Šä¿¡æ¯æŒ‰ä¸Šä¸‹æ–‡æ’åˆ—é¡ºåºæå–ï¼Œå…·ä½“æ—¶é—´éœ€ä»¥å…¬å‘ŠåŸæ–‡ä¸ºå‡†ã€‚å»ºè®®é€šè¿‡æä¾›çš„é“¾æ¥è·å–æ›´è¯¦ç»†å†…å®¹ã€‚"
     ]
    }
   ],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": context_chain, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "# res = rag_chain.invoke(query)\n",
    "# res\n",
    "for s in rag_chain.stream(query):\n",
    "    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e24e17b4-f356-43aa-8bbc-74d33b5aa476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI learns patterns from data to make predictions or decisions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=\"AIzaSyC_Zr5GS7vQNxOP8UNHeuPteCWHMR8QlVI\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\", contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8143c-3c9c-4894-940c-41e50ada379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "def doc_text_formatting(query, docs):\n",
    "    query_words = list(set(jieba.lcut(query)))  # ä¸­æ–‡åˆ†è¯\n",
    "    formatted_texts = []\n",
    "\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            highlighted = doc.get('text')\n",
    "        except:\n",
    "            highlighted = doc.text\n",
    "        for word in query_words:\n",
    "            if not word.strip():\n",
    "                continue\n",
    "            highlighted = re.sub(\n",
    "                re.escape(word),\n",
    "                f\"<span style='color:red'>{word}</span>\",\n",
    "                highlighted\n",
    "            )\n",
    "        formatted_texts.append(highlighted)\n",
    "    return formatted_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2047a-dbf4-4dca-bda0-8bf3fd920aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dense_embeddings = query_embeddings['dense_vecs']\n",
    "query_sparse_embeddings = query_embeddings.get('lexical_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fca140-8b46-48cd-93ab-139247da987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_results = hybrid_search(\n",
    "        col,\n",
    "        query_dense_embeddings[0],\n",
    "        query_sparse_embeddings[0],\n",
    "        sparse_weight=1.0,\n",
    "        dense_weight=1.0,\n",
    "        limit=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665f1336-5841-4d7d-9961-e18a29c3c1eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(\"### ğŸ”„ **Hybrid Search Results:**\"))\n",
    "formatted_results = doc_text_formatting(query, hybrid_results)\n",
    "for i, result in enumerate(formatted_results):\n",
    "    display(Markdown(f\"para_id: {hybrid_results[i].get('para_id')}\"))\n",
    "    display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb3d30d-5315-43e3-a9e5-bc44061c62da",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_rerank_results = rerank(hybrid_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9171e59-dfd4-4855-9896-b8e4bb04d2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_rerank_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db40210c-7373-4d4c-af95-4c13cc2285de",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"### ğŸ”„ **Reranked Hybrid Search Results:**\"))\n",
    "formatted_results = doc_text_formatting(query, hybrid_rerank_results)\n",
    "for i, result in enumerate(formatted_results):\n",
    "    display(Markdown(f\"para_id: {hybrid_results[i].get('para_id')}\"))\n",
    "    display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ff8a0d-cf68-450a-b1bd-806c4c808580",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ebed7-bded-4959-86dc-9ab705fe8e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_sub_chain = (\n",
    "    {\"context\": RunnableLambda(hybrid_results) | RunnableLambda(format_docs), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bc1a54-ded1-42ea-a93b-f78ee97d879a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda-eval] *",
   "language": "python",
   "name": "conda-env-Anaconda-eval-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
